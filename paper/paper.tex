% Paper template for TAR 2016
% (C) 2014 Jan Šnajder, Goran Glavaš, Domagoj Alagić, Mladen Karan
% TakeLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2016}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{makecell}
\usepackage[hyphens]{url}

\title{Token-level author diarization using clustering of stylistic contexts}

\name{Ivan Grubišić, Milan Pavlović, Author3} 

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\ 
\texttt{\{ivan.grubisic, milan.pavlovic, \}@fer.hr}\\
}
          
         
\abstract{ 
This document provides the instructions on formatting the TAR system description paper in \LaTeX{}. This is where you write the abstract (i.e., summary) of the work you carried out within the project. The abstract is a paragraph of text ranging between 70 and 150 words. This document provides the instructions on formatting the TAR system description paper in \LaTeX{}. This is where you write the abstract (i.e., summary) of the work you carried out within the project. The abstract is a paragraph of text ranging between 70 and 150 words.
}

\begin{document}

\maketitleabstract

\section{Introduction}
In this paper we will focus on the author diarization task proposed on PAN 2016 competition\footnote{\texttt{https://tinyurl.com/y9m4zntm}}. The aim of this task is to decompose a document into its authorial parts, i.e. to split a text into segments and assign an author to every segment \citep{koppel-2011,aldebei-2015}. This is one of the unsupervised variants of a well known authorship attribution problem since text samples of known authorship are not available \citep{rosso-2016}. As we will describe, in two out of three subtasks of this task only a correct number of authors for a given document is known.

The simplest variant of authorship attribution problem is about finding the most likely author for a given document, from a set of candidate authors whose authentic writing examples are available \citep{stamatatos-2009a,stein-2011,ding-2016}. Such descirbed problem can be tackeled with supervised machine learning techniques as a single-label multiclass text classification problem, where one class represents one author \citep{stamatatos-2009a}. 

Authorship attribution problem is also known as authorship identification and it is a part of authorship analysis \citep{stamatatos-2009a,ding-2016}. Authorship analysis is a field of stylometry and studies information about the authorship of a document, based on features derived from that document \citep{layton-2013}. Moreover, stylometry analyzes literary style with statistical methods \citep{stein-2011}.	

\citet{rosso-2016} divided PAN 2016 author diarization task into three subtasks. First subtask is traditionally called intrinsic plagiarism detection (IPD). The goal of this task is to find plagiariarized parts of a document in which 70\% of text is written by main author and the rest by one or more other authors. The term \textit{intrinsic} means that a decision whether plagiarized parts exist has to be made only by analysing a given document, without any comparisons with external sources. In the rest of the paper we refer to this subtask as a task \textit{a}.

\citet{rosso-2016} divided PAN 2016 author diarization task into three subtasks. First subtask is traditionally called intrinsic plagiarism detection (IPD). The goal of this task is to find plagiariarized parts of a document in which at least $70\%$ of text is written by main author and the rest by one or more other authors. The term \textit{intrinsic} means that a decision whether plagiarized parts exist or not has to be made only by analysing a given document, without any comparisons with external sources. In the rest of the paper we refer to this subtask as a task \textit{a}.

\begin{table}
	\caption{Basic characteristics of train datasets. * represents that there is a true author and plagiarism segments which don't have to originate from a single author.}
	\label{table-1}
	\begin{center}
		\begin{tabular}{cccc}
			\toprule
			Task & \thead{Number of \\ documents} & \thead{Average \\ length (in tokens)} & \thead{(min, max)\\authors} \\
			\midrule
			Task \textit{a} & 71 & 1679 & (2, 2)*\\
			Task \textit{b} & 55 & 3767 & (2, 10)\\
			Task \textit{c} & 54 & 3298 & (2, 10)\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

Other two subtasks are more related to the general task of author diarization. In the second subtask we need to segment a given document and group identified segments by author. In the rest of the paper we refer to the second subtask as a task \textit{b}. Third subtask differs from the second one in the fact that exact number of authors is unkown. In the rest of the paper we refer to the third subtask as a task \textit{c}.

For all three subtasks a different training datasets are publicly available\footnotemark[1]. \citet{rosso-2016} explain that they are collections of various documents which are part of Webis-TRC-12 dataset \citep{potthast-2013}. Every document in that dataset is constructed from texts of various search results (i.e. authors) for one of the 150 topics in total. By varying different parameters such as the number and proportion of the authors, places in a document where an author switch occurs (between words, sentences or paragraphs), three training and test datasets were generated \citep{rosso-2016}. Test datasets are currently not publicly available and we could not use them for evaluation of our approach. Some basic characterisics of training datasets are shown in Table \ref{table-1}.

% reference+
% authorship analysis+, 
% intrinsic plagiarism detection, author diarization+
% PAN 2016 author diarization task+
% dataset+

\section{Related work}
The basic assumption in authorship analysis is that texts of different authors are mutualy separable because each author has more or less unique writing style \citep{stamatatos-2009a,ding-2016}. More precisely, \citet{koppel-2009} explain that methods used in authorship analysis must be able to distinguish writing styles, but also tolerate shallow differences inside the same style because an author's stylistic habbits can conciously or unconciously vary over time. Therefore, most of related work tries to find the better features and methods which writing style will be quantified and measured with.

\citet{zu-2006} manually created a labeled corpus of plagiarized documents and used it for intrinsic plagiarism detection task. They used average sentence length, part of speech tags, average stopword number and the averaged word frequency class as input features for their linear discriminant analysis and suport vector machine (SVM) models. They approached that task in a supervised fashion. 

\citet{stamatatos-2009b} created a feature vector of normalized occurence of character tri-grams in the whole document. That vector represented a document's profile. Using a sliding window of fixed length he created same profiles for every window and compared them with the profile of the whole document. Result of comparison was an output from a style change function whose peaks were indicators of place in a document where style change occurs. All values above the predefined passage criterion were considered a result of plagiarism. That approach was unsupervised.

\citet{rahman-2015} classified sections of documents from PAN 2011 dataset with the help of SVM. Those sections were again obtanined by sliding a window of fixed length over the document. He also proposed new kind of information theoretical features - entropy, relative entropy, correlation coefficient and n-gram frequency class calculated from character tri-gram frequency profiles of each window and the whole document. He also used function word bi-gram and tri-gram frequency profiles with 1, 2, 3 and 4 skips. The value of style change function introduced by \citet{stamatatos-2009b} was also incorporated in feature vectors.

\citet{stein-2011} defined IPD as the one class classification problem where the text of main author belongs to a one target class and the rest are outliers. To find them, they estimated probability distributions of various stylistic features for the target class and outliers. Then a naive Bayes' algorithm was applied to feature vectors whose values lie outside the predefined uncertainity intervals. An additional outlier post-processing methods were also tested. The most successful was the unmasking technique described by \citet{koppel-2009}. The main sense of unmasking is to iteratively remove the best features that distinguish two classes and observe the speed with which cross-validation accuracy of again trained classifier drops. If the drop is slow and smooth, the outliers are indeed outliers because after \textit{n} iterations of removing discriminative features they are still separable from the main auhor's work.

 \citet{koppel-2011} used two staged approach in clustering of pre-segmented mixed biblical text written by two authors. First they used normalized cuts algorithm with cosine similarity to obtain initial clusters of segments which were represented only by normalized counts of synonymes from Hebrew synsets. Samples from initial clusers were separated in core and non-core samples via an iterative procedure, and core ones were labeled. SVM classifier was used to classify non-core samples, but now a bag-of-words feature vectors were used. The whole approach resulted with very good clusters. They also tried this method on an unsegmented case. Text was first splitted in a way that minimizes doubly-represented synonyms in segments and the same procedure was repeated. The clustering performance was lower than in pre-segmented case.

\citet{brooke-2013} concluded that a very good initial segmentation of text, at least in poems written by T. S. Elliot, is needed for a good performance of their modified k-means algorithm in clustering of voices. Except often character, lexical and syntactic features, they used features such as average frequency in a large  external corpus \citep{brants-2006}. One of the most promising feature they considered is the centroid of 20 dimensional distributional vectors obtained by applying latent semantic analysis on a large web corpus \cite{landauer-1997}.

The works by \citet{kuznetsov-2016} and \citet{sittar-2016} were submitted on the PAN 2016 competition for three aforementioned tasks. For the task \textit{a}, \citet{kuznetsov-2016} trained a Gradient Boosting Regression Trees (GBRT) model on PAN 2011 dataset as a style change function used for threshold based outlier detection. Every sentence was vectorized using word frequencies, n-gram frequencies, punctuaiton symbols and the universal POS tags count, sentence length and mean length of sentence words. The final input to the model was concatenation of center sentence vector and ones from context of size $\pm2$. In task \textit{b} they used a Hidden Markov Model with Gaussian Emissions for document segmentation over the same sentence scores from task \textit{a}. To estimate the unknown number of authors \textit{n} in task \textit{c}, they chose \textit{n} from 2 to 20 which maximizes their cluster discrepancy measure $Q(n)$.

\citet{sittar-2016} used k-means algorthm to cluster \textit{ClustDist} scores of each sentence, where the number of groups was equal to the known number of authors in tasks \textit{a} and \textit{b}. In task \textit{c}, a number of groups was generated randomly. Although they defined a \textit{ClustDist} score for a single sentence as an average distance between current and every other sentence vector, which is again a similar concept like a style change function, in provided example they used only a sum of unknown distance measures. Fifteen features in total were used for sentence vectorization, including average word and sentence lengths, count and ratios of characters, digits upperpase letters, spaces and tabs. 

The most of described approaches combine supervised and usupervised methods and operate on the level of longer text segments or sentences. We were inspired by \citet{brooke-2013} who said that a more radical approach would not separate those tasks in segmentation and clustering steps, but rather build an authorial segments that would also form good clusters. Since the style change in our tasks can occur even between two tokens in a same sentence, we wanted our model should be able to work on the token level. Instead of clustering tokens directly, we decided to cluster their vectorized stylistic contexts because they obviously contain more valuable sylistic information than tokens alone.

\section{Author diarization and intrinsic plagiarism detetion}
% describe the problem
% define formally: document, segmentation, segment, author assignement (label)
% define evaluation metrics

Let $\Delta$ be the domain of documents. We define a document $D\in \Delta$ as a finite sequence of tokens $(t_i)_{i=1}^n$, where $n$ can differ among documents. Given a document, each of its tokens is unique and defined by its character sequence and position in the document. Therefore, a document can be equivalently represented by its set of tokens $T_D=\{t_i\}_{i=1}^n$.

For each document, there is a corresponding mapping to a sequence of labels $(a_i)_{i=1}^n$ that are representing groupings of tokens by authors. The labels $a_i$ are indices of authors of the document. Each token $t_i\in T_D$ is assigned a document-level label $a_i \in \{1..c\}$ associating it to one of $c$ authors. The exact value of the label is not important. It is only required that all tokens corresponding to the same author have the same label. Therefore, there are $m!$ equivalent such mappings given a document. In the case of intrinsic plagiarism detection, there are only $2$ labels: $0$ representing the main author, and $1$ representing plagiarized text.

Equivalently, the codomain of the mapping can also be defined as a set of segmentations $\Sigma$. A segmentation $S\in \Sigma$ is a minimal set of segments, where each segment $s$ represents a set of consecutive tokens $\{t_i\}_{i=i_1}^{i_2}$ where each is assigned the same label. For a segmentation to be valid, the segments must cover all terms in the document and not overlap:
\begin{equation}
	\bigcup_{s\in S}s = T_D  \wedge \bigcap_{s\in S}s = \{\}.
\end{equation}
The correct mapping of a documents to the corresponding segmentations will be denoted with $\sigma: \Delta\rightarrow\Sigma$.

Let $\mathcal{D} \subset \Delta\times\Sigma$ be a dataset consisting of a finite set of pairs of documents and corresponding segmentations., i.e. $\mathcal{D} = \{\left(D_i, \sigma(D_i)\right)\}_{i=1}^N$. The goal is to find the model $\hat{\sigma}$ that best approximates the correct mapping $\sigma$, i.e. makes good predictions given unseen documents.

\subsection{Evaluation measures}

For evaluation of intrinsic plagiarism detection, \citet{stein-2010} define multiple measures for different aspects of a system's perfromance. The main measures are binary macro-averaged nad micro-averaged precision ($P$), recall (R) and $F_1$-score. For evaluating author diarization, we use \emph{BCubed} precision, recall and $F_1$ measures described by \citet{amigo-2009}, which are specialized for evaluation of clustering results. The same measures were used for evaluation on the PAN 2016 competition \citep{rosso-2016}.

Let $l$ be a function that associates lengths in characters to segments. Specially, $l(\{\}) = 0$. For notational convenience, we also use $l$ to denote the sum of lengths of all segments in a set of segments: $l(S) = \sum_{s\in S} l(s),$ where $S$ is as set of segments. Given a document $D$, let $S_\mathrm{p} \subseteq \sigma(D)$ be a set of all true plagiarism segments of the document and $\hat{S}_\mathrm{p} \subseteq \hat{\sigma}(D)$ the segments predicted as plagiarism by the  model. With ${S_\mathrm{tp} = \bigcup_{(s,\hat{s})\in S_\mathrm{p}\times\hat{S}_\mathrm{p}} l(s\cap\hat{s})}$, the micro-averaged evaluation measures for intrinsic plagiarism detection are defined as follows:
\begin{align}
P_\mathrm{\mu} &= \frac{l(\hat{S}_\mathrm{tp})}{l(\hat{S}_\mathrm{p})}, \\
R_\mathrm{\mu} &= \frac{l(\hat{S}_\mathrm{tp})}{l(S_\mathrm{p})}, \\
F_\mathrm{\mu} &= \frac{2}{P_\mathrm{\mu}^{-1}+R_\mathrm{\mu}^{-1}}.
\end{align}
The macro-average evaluation measures treat all plagiarism segments as equally important and are not affected by their lengths:
\begin{align}
P_\mathrm{M} &= \frac{1}{|\hat{S}_\mathrm{p}|}
	\sum_{\hat{s}\in\hat{S}_\mathrm{p}}
		\frac{{\sum_{s\in S_\mathrm{p}} l(s\cap\hat{s})}}{l(\hat{s})}, \\
R_\mathrm{M} &= \frac{1}{|S_\mathrm{p}|}
	\sum_{\hat{s}\in S_\mathrm{p}}
		\frac{{\sum_{s\in \hat{S}_\mathrm{p}} l(s\cap\hat{s})}}{l(s)}, \\
F_\mathrm{M} &= \frac{2}{P_\mathrm{M}^{-1}+R_\mathrm{M}^{-1}}.
\end{align}

In author diarization document segments have to be clustered into $c$ clusters, where $c$ is the number of authors that may or may not be known to the system. We divide the segments from the true segmentation $S$ and the predicted segmentation $\hat{S}$ each into sets of segments $S_i$, $i=1..c$ and $\hat{S}_j, j=1..\hat{c}$, where $c$ is the true number of authors, and $\hat{c}$ the predicted number of authors. We use the following \emph{BCubed} measures for evaluation:
\begin{align}
P_\mathrm{B^3} &= \sum_{i=1}^c \frac{1}{l(S_i)}\sum_{j=1}^{\hat{c}}
	\sum_{(s,\hat{s})\in S_i\times \hat{S}_j} l(s\cap \hat{s})^2 \\
R_\mathrm{B^3} &= \sum_{j=1}^{\hat{c}} \frac{1}{l(\hat{S}_j)}\sum_{i=1}^{c}
	\sum_{(s,\hat{s})\in S_i\times \hat{S}_j} l(s\cap \hat{s})^2 \\
F_\mathrm{B^3} &= \frac{2}{P_\mathrm{B^3}^{-1}+R_\mathrm{B^3}^{-1}}.
\end{align}


\section{The proposed approach}
% clustering, sliding window
% pipeline: preprocessing (tokenization) -> basic features -> transformed features (including scaling) -> clustering
% define the pipeline formally P = (f_p: d\times->t, f_b: t->\phi, f_t: \phi->\phi', c: \phi'->labels, f_s: labels\times d->s)
% describe GroupRepel loss, describe AutoKMeans, 
% clustering metric

Our approach can generally be described as a pipeline $P = (f_\mathrm{b}:\Delta\rightarrow \Phi_{n_b}, f_\mathrm{t}:\Phi_{n_b}\rightarrow \Phi_{n_t}, f_\mathrm{c}:\Phi_{n_t}\rightarrow C)$. Here $\Delta$ is the tokenized document domain as defined in section ((((((($\circ$))))))). $\Phi_{n_b}$ and $\Phi_{n_t}$ are sets of variable-length sequences of feature vectors with dimension $n_b$ and $n_t$ respectively. $n_b$ and $n_d$ can either be fixed or depend on the document being processed. $C$ is the set of all sequences of length $n$ (the number of tokens in the document) with elements being indices of authors/clusters.

The basic feature extractor denoted with $f_\mathrm{b}$ is used to extract stylistic features from the contexts of all tokens. If $D$ is a document with $n$ tokens, the basic feature extractor outputs a sequence of $n$ feature vectors, each vector representing the context of one token. $n_\mathrm{b}$ denotes the dimension of those vectors. The next step in the pipeline is the feature transformation $f_\mathrm{t}$ that maps the basic features to a space that they can be better clustered in. The dimension of the new feature space $n_\mathrm{t}$ generally doesn't equal $n_\mathrm{b}$. The final step in the pipeline is clustering denoted with $f_\mathrm{c}$. The clustering algorithm clusters tokens, each cluster representing an author. Depending on the task, The clustering algorithm can either be given a known number of authors, or try to predict it.

The following steps are done in predicting the segmentation: (1) raw text is tokenized giving a sequence of tokens $D\in\Delta$, (2) features are extracted for all tokens and their contexts, giving a sequence of feature vectors $\phi_\mathrm{t} = (\mathbf{t}_i)_{i=1}^n = (f_\mathrm{t}\circ f_\mathrm{b})(D)$, (3) the tokens are clustered based on $\phi_\mathrm{t}$, giving a sequence of author labels $(a_i)_{i=1}^{n}$, where $n=|T_D|$, and (4) a segmentation $\hat{S}$ is generated based on the document $D$ and the sequence of predicted author labels.

% details for each component
\paragraph{Tokenization.} Tokenization.

\paragraph{Basic feature extraction.} Differences between the 2 model variants.

basic features

\paragraph{Feature transformation.} Differences between the 2 model variants. Training.

identity

scaling

concatenation with squared features

transformation trained via SGD

Let $(\mathbf{b}_i)_{i=1}^n = f_\mathrm{b}(D)$ be the sequence of basic feature vectors with elements from $\mathbb{R}^{n_b}$. Let $(a_i)_{i=1}^n$ be the sequence of true author labels with elements from $\{1..c\}$. We want to maximize the \emph{clusterability} of the feature vectors obtained by the feature transformation $T$. Let $(\mathbf{t}_i)_{i=1}^n =  f_\mathrm{t}((\mathbf{b}_i)_{i=1}^n) = (T(\mathbf{b}_i))_{i=1}^n$ be the sequence of transformed feature vectors with elements from $\mathbb{R}^{n_t}$.


\paragraph{Clustering.}

clustering algorithms

k-means HAC DBSCAN auto-k-means

\paragraph{Segmentation generation.} something short


features

differences: fixed features + transformation vs document-dependent features

\section{Experimental results}

1
baselines

Mention cofidences.

\subsection{Intrinsic plagiarism detection}

setup
results
don't forget to describe tolerance

\subsection{Author diarization with known numbers of authors}

setup

\subsection{Author diarization with unknown numbers of authors}

setup

\section{Conclusion}

Conclusion is the last enumerated section of the paper. It should not exceed half of a column and is typically split into 2--3 paragraphs. No new information should be presented in the conclusion; this section only summarizes and concludes the paper.

\section{Further work}
% more systematic parameter choice, model development
% mention possible applicability to author clustering and extrinsic plagiarism detections

\section*{Acknowledgements}



If suitable, you can include the \textit{Acknowledgements} section before inserting the literature references  in order to thank those who helped you in any way to deliver the paper, but are not co-authors of the paper.

\begin{table}
	\caption{This is the caption of the table. Table captions should be placed \textit{above} the table.}
	\label{tab:narrow-table}
	\begin{center}
		\begin{tabular}{c|ccc|ccc}
			\toprule
			Model & $R_\mathrm{\mu}$ & $P_\mathrm{\mu}$ & $F_\mathrm{\mu}$ & $R_\mathrm{M}$ & $P_\mathrm{M}$ & $F_\mathrm{M}$\\
			\midrule
			Dummy & 0 & 1 & 2 \\
			One & 0 & 1 & 2 \\
			One & 0 & 1 & 2 \\
			\midrule
			One & 0 & 1 & 2 \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\bibliographystyle{tar2016}
\bibliography{tar2017}

\end{document}

